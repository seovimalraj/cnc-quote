# LiteLLM configuration
# Routes requests to different LLM providers (Ollama, OpenAI, Anthropic, etc.)

model_list:
  # Ollama models (assumes Ollama running on host at :11434)
  - model_name: qwen2.5-7b-instruct
    litellm_params:
      model: ollama/qwen2.5:7b-instruct
      api_base: http://host.docker.internal:11434
      
  - model_name: llava-7b
    litellm_params:
      model: ollama/llava:7b
      api_base: http://host.docker.internal:11434
      
  - model_name: bge-m3-embeddings
    litellm_params:
      model: ollama/bge-m3
      api_base: http://host.docker.internal:11434

  # Optional: OpenAI fallback (uncomment if you have API key)
  # - model_name: gpt-4
  #   litellm_params:
  #     model: gpt-4
  #     api_key: ${OPENAI_API_KEY}

# General settings
litellm_settings:
  # Caching
  cache: true
  cache_params:
    type: redis
    host: redis
    port: 6379
    
  # Logging
  success_callback: ["langfuse"]
  failure_callback: ["sentry"]
  
  # Rate limiting
  rpm: 1000
  tpm: 100000
  
  # Timeouts
  request_timeout: 600
  
# Router settings
router_settings:
  routing_strategy: latency-based-routing
  redis_host: redis
  redis_port: 6379
  
# Optional: Database for usage tracking
# database_url: ${DATABASE_URL}
