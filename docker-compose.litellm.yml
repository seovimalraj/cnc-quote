version: '3.8'

services:
  # LiteLLM proxy service - OpenAI-compatible API gateway
  litellm:
    image: ghcr.io/berriai/litellm:main-latest
    container_name: litellm-proxy
    ports:
      - "4000:4000"
    environment:
      # LiteLLM configuration
      - LITELLM_MASTER_KEY=${LITELLM_MASTER_KEY:-sk-1234}
      - LITELLM_LOG=INFO
      - DATABASE_URL=${LITELLM_DATABASE_URL:-postgresql://postgres:postgres@db:5432/litellm}
      
      # Ollama backend (assumes Ollama running on host)
      - OLLAMA_API_BASE=http://host.docker.internal:11434
      
      # Optional: OpenAI fallback (if you have API key)
      # - OPENAI_API_KEY=${OPENAI_API_KEY}
    volumes:
      - ./litellm-config.yaml:/app/config.yaml:ro
    command: 
      - "--config"
      - "/app/config.yaml"
      - "--port"
      - "4000"
    networks:
      - app-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:4000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
    depends_on:
      - db

  # Qdrant vector database (optional alternative to pgvector)
  qdrant:
    image: qdrant/qdrant:latest
    container_name: qdrant
    ports:
      - "6333:6333"
      - "6334:6334"
    environment:
      - QDRANT__SERVICE__GRPC_PORT=6334
    volumes:
      - qdrant-storage:/qdrant/storage
    networks:
      - app-network
    restart: unless-stopped

networks:
  app-network:
    driver: bridge

volumes:
  qdrant-storage:
    driver: local
